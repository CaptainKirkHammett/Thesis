{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/AlaGrine/LinkedIn_Job_Scraper_and_Matcher/tree/main/FLASK_app\n",
    "# pip install selenium \n",
    "# pip install beautifulsoup4\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import requests\n",
    "\n",
    "import time, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, re, sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Instanciate the chrome service\n",
    "chromedriver_path = 'C:\\OAMK\\Python Programs\\chromedriver-win64130\\chromedriver-win64\\chromedriver.exe'\n",
    "service = Service(executable_path=chromedriver_path)\n",
    "\n",
    "# 2. Instanciate the webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-search-engine-choice-screen\")\n",
    "driver = webdriver.Chrome(options=options, service=service)\n",
    "\n",
    "# 3. Open the LinkedIn login page\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "time.sleep(5) # waiting for the page to load\n",
    "\n",
    "# 4. Enter email address & password\n",
    "email_input = driver.find_element(By.ID, 'username')\n",
    "password_input = driver.find_element(By.ID, 'password')\n",
    "email_input.send_keys(\"xx@xx.com\")\n",
    "password_input.send_keys(\"xx\")\n",
    "\n",
    "# 5. Click the login button\n",
    "password_input.send_keys(Keys.ENTER)\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "List_Job_IDs = []\n",
    "\n",
    "# Create a function 'Scroll to the bottom'. \n",
    "\n",
    "# time.sleep() function is used to provide extra time for the webpage to load. \n",
    "# I used 120 seconds. If the 25 jobs have not loaded during this period, we can make adjust it and test again.\n",
    "\n",
    "def scroll_to_bottom(driver,sleep_time=120):\n",
    "    last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    while True:\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    time.sleep(sleep_time)  \n",
    "\n",
    "\n",
    "\n",
    "# Navigate to the first page (start=0) and scroll to the bottom of the page\n",
    "\n",
    "keywords = 'data'\n",
    "location = 'Finland'\n",
    "start = 0\n",
    "\n",
    "url = f'https://www.linkedin.com/jobs/search/?keywords={keywords}&location={location}&start={start}'\n",
    "url = requests.utils.requote_uri(url)\n",
    "driver.get(url)\n",
    "scroll_to_bottom(driver,sleep_time=120)\n",
    "\n",
    "\n",
    "# Get number of jobs found and number of pages:\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup.\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "try:\n",
    "    div_number_of_jobs = soup.find(\"div\",{\"class\":\"jobs-search-results-list__subtitle\"})\n",
    "    number_of_jobs = int(div_number_of_jobs.find('span').get_text().strip().split()[0])\n",
    "except:\n",
    "    number_of_jobs = 0\n",
    "    \n",
    "number_of_pages=math.ceil(number_of_jobs/25)\n",
    "print(\"number_of_jobs:\",number_of_jobs)\n",
    "print(\"number_of_pages:\",number_of_pages)\n",
    "\n",
    "\n",
    "\n",
    "# Get Job Ids present on the first page.\n",
    "\n",
    "def find_Job_Ids(soup):\n",
    "\n",
    "    Job_Ids_on_the_page = []\n",
    "    \n",
    "    job_postings = soup.find_all('li', {'class': 'jobs-search-results__list-item'})\n",
    "    for job_posting in job_postings:\n",
    "        Job_ID = job_posting.get('data-occludable-job-id')\n",
    "        Job_Ids_on_the_page.append(Job_ID)\n",
    "        # job_title = job_posting.find('a', class_='job-card-list__title').get_text().strip()\n",
    "        # location = job_posting.find('li', class_='job-card-container__metadata-item').get_text().strip()\n",
    "    \n",
    "    return Job_Ids_on_the_page    \n",
    "\n",
    "Jobs_on_this_page = find_Job_Ids(soup)\n",
    "List_Job_IDs.extend(Jobs_on_this_page)\n",
    "\n",
    "\n",
    "if number_of_pages>1:\n",
    "    \n",
    "    for page_num in range(1,number_of_pages):\n",
    "        print(f\"Scraping page: {page_num}\",end=\"...\")\n",
    "        \n",
    "        # Navigate to page\n",
    "        url = f'https://www.linkedin.com/jobs/search/?keywords={keywords}&location={location}&start={25 * page_num}'\n",
    "        url = requests.utils.requote_uri(url)\n",
    "        driver.get(url)\n",
    "        scroll_to_bottom(driver)\n",
    "\n",
    "        # Parse the HTML content of the page using BeautifulSoup.\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Get Job Ids present on the page.\n",
    "        Jobs_on_this_page = find_Job_Ids(soup)\n",
    "        List_Job_IDs.extend(Jobs_on_this_page)  \n",
    "        print(f'Jobs found:{len(Jobs_on_this_page)}')\n",
    "\n",
    "pd.DataFrame({\"Job_Id\":List_Job_IDs}).to_csv('Job_Ids.csv',index=False)\n",
    "\n",
    "## Close the browser and shut down the ChromiumDriver executable that\n",
    "# is started when starting the ChromiumDriver. \n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(html):\n",
    "    '''remove html tags from BeautifulSoup.text'''\n",
    " \n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    " \n",
    "    for data in soup(['style', 'script']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    " \n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "  \n",
    "job_url='https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{}'\n",
    "job={}\n",
    "list_jobs=[]\n",
    "\n",
    "for j in range(0,len(List_Job_IDs)):\n",
    "    print(f\"{j+1} ... read jobId:{List_Job_IDs[j]}\")\n",
    "\n",
    "    resp = requests.get(job_url.format(List_Job_IDs[j]))\n",
    "    soup=BeautifulSoup(resp.text,'html.parser')\n",
    "    # print(soup.prettify()) \n",
    "\n",
    "    job[\"Job_ID\"] = List_Job_IDs[j] \n",
    "    \n",
    "    try: \n",
    "        job[\"Job_txt\"] = remove_tags(resp.content)\n",
    "    except:\n",
    "        job[\"Job_txt\"] = None\n",
    "    \n",
    "    try:\n",
    "        job[\"company\"]=soup.find(\"div\",{\"class\":\"top-card-layout__card\"}).find(\"a\").find(\"img\").get('alt')\n",
    "    except:\n",
    "        job[\"company\"]=None\n",
    "\n",
    "    try:\n",
    "        job[\"job-title\"]=soup.find(\"div\",{\"class\":\"top-card-layout__entity-info\"}).find(\"a\").text.strip()\n",
    "    except:\n",
    "        job[\"job-title\"]=None\n",
    "\n",
    "    try:\n",
    "        job[\"level\"]=soup.find(\"ul\",{\"class\":\"description__job-criteria-list\"}).find(\"li\").text.replace(\"Seniority level\",\"\").strip()\n",
    "    except:\n",
    "        job[\"level\"]=None\n",
    "\n",
    "    try:\n",
    "        job[\"location\"]=soup.find(\"span\",{\"class\":\"topcard__flavor topcard__flavor--bullet\"}).text.strip()\n",
    "    except:\n",
    "        job[\"location\"]=None\n",
    "\n",
    "    try:\n",
    "        job[\"posted-time-ago\"]=soup.find(\"span\",{\"class\":\"posted-time-ago__text topcard__flavor--metadata\"}).text.strip()\n",
    "    except:\n",
    "        job[\"posted-time-ago\"]=None\n",
    "\n",
    "    try:\n",
    "        nb_candidats = soup.find(\"span\",{\"class\":\"num-applicants__caption topcard__flavor--metadata topcard__flavor--bullet\"}).text.strip()\n",
    "        nb_candidats = int(nb_candidats.split()[0])\n",
    "        job[\"nb_candidats\"]= nb_candidats\n",
    "    except:\n",
    "        job[\"nb_candidats\"]=None\n",
    "\n",
    "    list_jobs.append(job)\n",
    "    job={}\n",
    "\n",
    "# create a pandas Datadrame\n",
    "jobs_df = pd.DataFrame(list_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "\n",
    "#jobs_df.to_csv(\"jobsFrame.csv\")\n",
    "#jobs_df.to_excel(\"jobsFrame2.xlsx\")\n",
    "jobs_df.to_excel(\"jobsFrame4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import requests\n",
    "\n",
    "import time, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, re, sys\n",
    "import warnings\n",
    "import selenium\n",
    "import bs4\n",
    "\n",
    "print(pd.__version__)\n",
    "print(selenium.__version__)\n",
    "print(bs4.__version__)\n",
    "print(requests.__version__)\n",
    "print(np.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
